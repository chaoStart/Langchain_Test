
import pandas as pd
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk

# 1. è¿æ¥ Elasticsearch
es = Elasticsearch("http://localhost:9200", verify_certs=False)
index_name = 'manual_storage_es_big_chunk'

# 2.è¯»å–è‡ªå·±ç¼–å†™çš„.xlsxæ–‡ä»¶ï¼Œå¹¶å­˜å…¥åˆ°esæ•°æ®åº“
# df_docs =pd.read_excel('2025å¹´æ—¥æŒ‡æ ‡å¤§å”æ±Ÿè‹å…¬å¸.xlsx', sheet_name=Sheet_name, engine='openpyxl')
df_docs =pd.read_excel('æ‰‹åŠ¨åˆ›å»ºçš„å­˜å‚¨æ•°æ®_æ•´å—.xlsx', engine='openpyxl')
print(df_docs.head(5))

def new_clear_es(index_name):
    es.indices.create(
        index=index_name,
        body={
            "settings": {
                "analysis": {
                    "analyzer": {
                        "ik_max_word_analyzer": {
                            "type": "custom",
                            "tokenizer": "ik_max_word"
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    "company_name": {
                        "type": "text",
                        "analyzer": "ik_max_word",
                        "search_analyzer": "ik_smart"
                    },
                    "data_month": {
                        "type": "text",
                        "analyzer": "ik_max_word",
                        "search_analyzer": "ik_smart"
                    },
                    "sheet_name": {
                        "type": "text",
                        "analyzer": "ik_max_word",
                        "search_analyzer": "ik_smart"
                    },
                    "row_name": {
                        "type": "text",
                        "analyzer": "ik_max_word",
                        "search_analyzer": "ik_smart"
                    },
                    "column_name": {
                        "type": "text",
                        "analyzer": "ik_max_word",
                        "search_analyzer": "ik_smart"
                    },
                    # "content": {
                    #     "type": "text",
                    #     "analyzer": "ik_max_word",
                    #     "search_analyzer": "ik_smart"
                    # }
                }
            }
        }
    )
    print("âœ… å·²åˆ›å»ºç´¢å¼•å¹¶å¯ç”¨ IK åˆ†è¯å™¨")
    # 4. æ‰¹é‡æ„é€ å†™å…¥æ•°æ®
    actions = []
    for _, row in df_docs.iterrows():
        doc = {
            "company_name": str(row["Company_name"]) if pd.notna(row["Company_name"]) else "",
            "data_month": str(row["Data_month"]) if pd.notna(row["Data_month"]) else "",
            "sheet_name": str(row["Sheet_name"]) if pd.notna(row["Sheet_name"]) else "",
            "row_name": str(row["Row_name"]) if pd.notna(row["Row_name"]) else "",
            "column_name": str(row["Column_name"]) if pd.notna(row["Column_name"]) else "",
            # "content": str(row["Content"]) if pd.notna(row["Content"]) else ""
        }
        actions.append({"_index": index_name, "_source": doc})

    bulk(es, actions)
    print(f"âœ… æˆåŠŸå†™å…¥ {len(actions)} æ¡æ–‡æ¡£åˆ° Elasticsearch ç´¢å¼•ï¼š{index_name}")

# 3. åˆ›å»ºæ”¯æŒ multi_match çš„ç´¢å¼•ï¼ˆå« ik åˆ†è¯å™¨ï¼‰
if not es.indices.exists(index=index_name):
    new_clear_es(index_name)
else:
    # åˆ é™¤å·²æœ‰ç´¢å¼•ï¼ˆè°¨æ…æ“ä½œï¼‰
    print("âš ï¸ ç´¢å¼•å·²å­˜åœ¨ï¼ï¼ï¼")
    # es.indices.delete(index=index_name)
    # new_clear_es(index_name)


# 5.ç®€å•æµ‹è¯•æ˜¯å¦å¯ä»¥æŸ¥è¯¢æ£€ç´¢
response = es.search(index=index_name, body={
    "query": {
        "match": {
            "column_name": "å¹³å‡å‘ç”µè®¾å¤‡å®¹é‡å’Œæœ¬æœˆæ•°",
        }
    },
    "_source": ["sheet_name"],  # åªè¿”å› sheet_name å­—æ®µ
    "size": 15,
})
sheet_names = [hit["_source"]["sheet_name"] for hit in response["hits"]["hits"]]
print("âœ… æ ¹æ®åˆ—åè¿”å›æŸ¥è¯¢çš„æ£€ç´¢ä¿¡æ¯",sheet_names)


# 6. ä½¿ç”¨ multi_match æŸ¥è¯¢ row_nameã€column_name å’Œ content å­—æ®µï¼Œè¿”å›åŒ¹é…çš„ sheet_name
def search_sheet_name_by_keywords(keywords, top_k=5):
    query_body = {
        "query": {
            "multi_match": {
                "query": keywords,
                # "fields": ["row_name", "column_name", "content"],
                "fields": ["row_name", "column_name"],
                # "type": "most_fields",  # å¯ä»¥æ”¹ä¸º "most_fields" æˆ– "phrase" è§†å…·ä½“æ•ˆæœè°ƒæ•´
                "type": "cross_fields",  # å¯ä»¥æ”¹ä¸º "most_fields" æˆ– "phrase" è§†å…·ä½“æ•ˆæœè°ƒæ•´
                "analyzer": "ik_max_word"
            }
        },
        "_source": ["sheet_name"],  # åªè¿”å› sheet_name å­—æ®µ
        "size": top_k
    }

    response = es.search(index=index_name, body=query_body)
    sheet_names = [hit["_source"]["sheet_name"] for hit in response["hits"]["hits"]]
    return sheet_names

# ç¤ºä¾‹è°ƒç”¨ï¼šæ ¹æ®å…³é”®è¯æŸ¥æ‰¾å·¥ä½œè¡¨åç§°
question = "å‘ç”µå‚ç”¨ç”µé‡çš„æœ¬å¹´ç´¯è®¡æ•°"
matched_sheets = search_sheet_name_by_keywords(question)
print("ğŸ” æ ¹æ®è¡Œå+åˆ—å åŒ¹é…çš„å·¥ä½œè¡¨åç§°ï¼š", matched_sheets)
print("------------------------------------------------")
# éªŒè¯å½“å‰ç´¢å¼•ä¸­æ˜¯å¦æœ‰æ–‡æ¡£
print("å½“å‰ç´¢å¼•æ–‡æ¡£æ€»æ•°ï¼š", es.count(index=index_name)["count"])